#### BEGIN CONFIGURATION ####

# Where is the SpatialRestraint located?
executable_path = '/mnt/home/fergu358/research/rogue_cell/SpatialRestraint/SpatialRestraint'
# Where should we save *actual* output data (from the SpatialRestraint app)
scratch_dir = '/mnt/gs18/scratch/users/fergu358/rogue_cell/SpatialRestraint_timing/512/'
# Where should we save the slurm scripts generated by this script?
job_dir = '/mnt/home/fergu358/research/rogue_cell/SpatialRestraint/jobs/'

# SpatialRestraint config options
    # Variables not shown as lists will be treated as constant across all runs
    # Variables that are lists are used such that *every* combination is generated
ancestor_ones_list = list(range(81, 86))
unrestrained_cost_list = [0]
mc_size_list = [512]
gen_num = 0
mut_rate = 0.2
pop_size = 10000
samples_num = 10
# Any extra flags to send to SpatialRestraint
extra_flags = '-o -v -P'
# How many copies of each job should we run?
job_copy_start = 0
job_copy_stop = 100
# This is a simple offset for the job id. If first batch is 0-1000, set this as 1000 to get 1000-2000
job_id_start = 9000000
# We can crank this up to have multiple tasks per job (one job file for several runs)
# Decreases submission time to the queue, but must submit all tasks at once
tasks_per_job = 1

#### END CONFIGURATION ####

# Calculate the number of jobs we expect. 
    # We also print the actual number of jobs at the end. They should alwaya match. 
total_jobs = \
    (job_copy_stop - job_copy_start) * \
    len(ancestor_ones_list) * \
    len(mc_size_list) * \
    len(unrestrained_cost_list)
print('Expecting ' + str(total_jobs) + ' jobs...')
final_job_id = job_id_start + total_jobs
num_digits = len(str(final_job_id))


num_jobs = 0
cur_job_id = job_id_start
# Iterate through each combination of configuration variables
# TODO: Make this less awful. Something like the combinations thing Charles added to Empirical
for ancestor_ones in ancestor_ones_list:
    for mc_size in mc_size_list:
        for unrestrained_cost in unrestrained_cost_list:
            for job_copy_id in range(job_copy_start, job_copy_stop):
                num_jobs += 1
                cur_job_id += 1
                # Embed configuration options into filename
                job_id_str = str(cur_job_id)
                job_id_str = '0' * (num_digits - len(job_id_str)) + job_id_str
                filename_prefix = job_id_str + '_spatial_restraint' + \
                    '__ONES_' + str(ancestor_ones) + \
                    '__MCSIZE_' + str(mc_size) + \
                    '__COST_' + str(unrestrained_cost) + \
                    '__NUM_' + str(job_copy_id) 
                # Write slurm job file using current configuration options
                with open(job_dir + filename_prefix + '.sb', 'w') as fp_job:
                    fp_job.write('#!/bin/bash --login' + '\n')
                    fp_job.write('' + '\n')
                    # Change the time per job here!
                    fp_job.write('#SBATCH --time=2:00:00' + '\n')
                    fp_job.write('#SBATCH --nodes=1' + '\n')
                    fp_job.write('#SBATCH --ntasks=1' + '\n')
                    fp_job.write('#SBATCH --cpus-per-task=1' + '\n')
                    fp_job.write('#SBATCH --mem-per-cpu=1G' + '\n')
                    fp_job.write('#SBATCH --job-name Spatial_Restraint' + '\n')
                    fp_job.write('#SBATCH --array=1-' + str(tasks_per_job) + '\n')
                    fp_job.write('#SBATCH --output='  + \
                        scratch_dir + filename_prefix + '_%a__slurm.out' + \
                        '\n')
                    fp_job.write('' + '\n')
                    fp_job.write('module purge' + '\n')
                    fp_job.write('module load GCC/9.1.0-2.32' + '\n')
                    fp_job.write('' + '\n')
                    fp_job.write('mkdir -p ' + scratch_dir + filename_prefix + '\n') 
                    fp_job.write('' + '\n')
                    fp_job.write('echo "' + executable_path)
                    fp_job.write(' -a ' + str(ancestor_ones))
                    fp_job.write(' -c ' + str(mc_size))
                    fp_job.write(' -g ' + str(gen_num))
                    fp_job.write(' -m ' + str(mut_rate))
                    fp_job.write(' -M ' + scratch_dir + filename_prefix + \
                        '/${SLURM_ARRAY_TASK_ID}_multicell.dat')
                    fp_job.write(' -d ' + str(samples_num))
                    fp_job.write(' -u ' + str(unrestrained_cost))
                    fp_job.write(' ' + extra_flags + ' ')
                    fp_job.write('"\n')
                    fp_job.write('time ' + executable_path)
                    fp_job.write(' -a ' + str(ancestor_ones))
                    fp_job.write(' -c ' + str(mc_size))
                    fp_job.write(' -g ' + str(gen_num))
                    fp_job.write(' -m ' + str(mut_rate))
                    fp_job.write(' -M ' + scratch_dir + filename_prefix + \
                        '/${SLURM_ARRAY_TASK_ID}_multicell.dat')
                    fp_job.write(' -d ' + str(samples_num))
                    fp_job.write(' -u ' + str(unrestrained_cost))
                    fp_job.write(' ' + extra_flags)
                    fp_job.write('\n')
                    fp_job.write('\n')
                    fp_job.write('scontrol show job $SLURM_JOB_ID' + '\n')

print('Generated ' +  str(num_jobs) + '!')
